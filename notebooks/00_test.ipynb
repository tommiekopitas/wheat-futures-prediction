{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6ef3f8",
   "metadata": {},
   "source": [
    "# France Wheat Futures Prediction - Complete Pipeline\n",
    "# \n",
    "# This notebook:\n",
    "# 1. Loads CY-Bench agricultural data (weather, vegetation, soil)\n",
    "# 2. Loads Euronext wheat futures prices\n",
    "# 3. Interpolates sparse data to daily resolution\n",
    "# 4. Creates t+20 day price prediction targets\n",
    "# 5. Comprehensive data exploration\n",
    "# 6. Saves processed dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "CYBENCH_DIR = Path(\"../data/raw/cybench/cybench-data/wheat/FR/\")\n",
    "FUTURES_PATH = Path(\"../data/raw/futures/milling_wheat_raw.csv\")\n",
    "OUTPUT_DIR = Path(\"../data/processed\")\n",
    "FIGURES_DIR = Path(\"../reports/figures\")\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "HORIZON = 20  # Predict price 20 trading days ahead\n",
    "SAMPLE_SIZE = 10000  # Sample size for quick testing\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"âœ“ Configuration set\")\n",
    "print(f\"  - Prediction horizon: t+{HORIZON} days\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Figures directory: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_cybench",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD CY-BENCH DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading CY-Bench France wheat data...\\n\")\n",
    "\n",
    "# 1. Yield data (ground truth for original task)\n",
    "yield_df = pd.read_csv(CYBENCH_DIR / \"yield_wheat_FR.csv\")\n",
    "print(f\"âœ“ Yield data: {yield_df.shape}\")\n",
    "print(f\"  Years: {yield_df['harvest_year'].min()}-{yield_df['harvest_year'].max()}\")\n",
    "print(f\"  Regions: {yield_df['adm_id'].nunique()}\")\n",
    "\n",
    "# 2. Weather data (daily)\n",
    "meteo_df = pd.read_csv(CYBENCH_DIR / \"meteo_wheat_FR.csv\")\n",
    "print(f\"\\nâœ“ Weather data: {meteo_df.shape}\")\n",
    "print(f\"  Columns: {list(meteo_df.columns)}\")\n",
    "\n",
    "# 3. Vegetation data (dekadal - 10-day periods)\n",
    "fpar_df = pd.read_csv(CYBENCH_DIR / \"fpar_wheat_FR.csv\")\n",
    "print(f\"\\nâœ“ fPAR data: {fpar_df.shape}\")\n",
    "\n",
    "# 4. Soil moisture (if available)\n",
    "try:\n",
    "    soil_df = pd.read_csv(CYBENCH_DIR / \"soil_moisture_wheat_FR.csv\")\n",
    "    print(f\"\\nâœ“ Soil moisture data: {soil_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸  Soil moisture data not found\")\n",
    "    soil_df = None\n",
    "\n",
    "# 5. Crop calendar (static - growing season dates)\n",
    "try:\n",
    "    calendar_df = pd.read_csv(CYBENCH_DIR / \"crop_calendar_wheat_FR.csv\")\n",
    "    print(f\"\\nâœ“ Crop calendar: {calendar_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸  Crop calendar not found\")\n",
    "    calendar_df = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample weather data:\")\n",
    "print(meteo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_futures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD FUTURES DATA\n",
    "# ============================================================\n",
    "\n",
    "def clean_futures(df):\n",
    "    \"\"\"Clean Investing.com downloaded data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = [c.lower().strip().replace(' ', '_').replace('.', '').replace('%', 'pct') \n",
    "                  for c in df.columns]\n",
    "    \n",
    "    # Parse date with multiple formats\n",
    "    df['date'] = pd.to_datetime(df['date'], format='mixed', dayfirst=False, errors='coerce')\n",
    "    \n",
    "    # If still NaT, try other formats\n",
    "    if df['date'].isna().any():\n",
    "        mask = df['date'].isna()\n",
    "        df.loc[mask, 'date'] = pd.to_datetime(df.loc[mask, 'date'], format='%d/%m/%Y', errors='coerce')\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    for col in ['price', 'open', 'high', 'low']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Rename price -> close\n",
    "    df = df.rename(columns={'price': 'close'})\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Loading futures data...\\n\")\n",
    "futures_df = pd.read_csv(FUTURES_PATH)\n",
    "futures_clean = clean_futures(futures_df)\n",
    "\n",
    "print(f\"âœ“ Futures data: {futures_clean.shape}\")\n",
    "print(f\"  Date range: {futures_clean['date'].min()} to {futures_clean['date'].max()}\")\n",
    "print(f\"  Price range: â‚¬{futures_clean['close'].min():.2f} - â‚¬{futures_clean['close'].max():.2f}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(futures_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def convert_date_format(df, date_col='date'):\n",
    "    \"\"\"Convert date column from int (YYYYMMDD) to datetime64.\"\"\"\n",
    "    df = df.copy()\n",
    "    if df[date_col].dtype == 'int64':\n",
    "        df[date_col] = pd.to_datetime(df[date_col].astype(str), format='%Y%m%d')\n",
    "    elif df[date_col].dtype != 'datetime64[ns]':\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def interpolate_to_daily(df, date_col='date', value_col='fpar', group_col='adm_id'):\n",
    "    \"\"\"\n",
    "    Interpolate sparse time series (dekadal/8-day) to daily resolution.\n",
    "    \"\"\"\n",
    "    print(f\"  Interpolating {value_col} to daily...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = convert_date_format(df, date_col)\n",
    "    \n",
    "    # Get date range\n",
    "    min_date = df[date_col].min()\n",
    "    max_date = df[date_col].max()\n",
    "    \n",
    "    print(f\"    Original: {len(df)} observations\")\n",
    "    print(f\"    Date range: {min_date.date()} to {max_date.date()}\")\n",
    "    \n",
    "    # Interpolate for each region\n",
    "    regions = df[group_col].unique()\n",
    "    interpolated = []\n",
    "    \n",
    "    for region in regions:\n",
    "        region_data = df[df[group_col] == region].copy()\n",
    "        region_data = region_data.set_index(date_col)\n",
    "        \n",
    "        # Create daily index\n",
    "        daily_index = pd.date_range(\n",
    "            region_data.index.min(), \n",
    "            region_data.index.max(), \n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        # Reindex and interpolate\n",
    "        region_daily = region_data[[value_col]].reindex(daily_index)\n",
    "        region_daily[value_col] = region_daily[value_col].interpolate(\n",
    "            method='linear',\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "        region_daily[group_col] = region\n",
    "        region_daily = region_daily.reset_index()\n",
    "        region_daily = region_daily.rename(columns={'index': date_col})\n",
    "        \n",
    "        interpolated.append(region_daily)\n",
    "    \n",
    "    result = pd.concat(interpolated, ignore_index=True)\n",
    "    print(f\"    Interpolated: {len(result)} observations\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_daily_targets(futures_df, horizon=20):\n",
    "    \"\"\"\n",
    "    Create target prices: for each trading day, get price at t+horizon.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating t+{horizon} targets...\")\n",
    "    \n",
    "    df = futures_df[['date', 'close']].copy()\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Shift to get future price\n",
    "    df[f'target_price_t{horizon}'] = df['close'].shift(-horizon)\n",
    "    df = df.rename(columns={'close': 'price_current'})\n",
    "    \n",
    "    # Calculate future return\n",
    "    df[f'target_return_t{horizon}'] = (\n",
    "        (df[f'target_price_t{horizon}'] / df['price_current']) - 1\n",
    "    ) * 100\n",
    "    \n",
    "    valid_targets = df[f'target_price_t{horizon}'].notna().sum()\n",
    "    print(f\"  Total trading days: {len(df)}\")\n",
    "    print(f\"  Valid targets: {valid_targets}\")\n",
    "    print(f\"  (Last {horizon} days have no target)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_daily_features(meteo_df, soil_moisture_df, fpar_daily):\n",
    "    \"\"\"\n",
    "    Merge all daily features into single dataframe.\n",
    "    \"\"\"\n",
    "    print(\"\\nMerging daily features...\")\n",
    "    \n",
    "    # Start with meteo as base\n",
    "    result = meteo_df.copy()\n",
    "    result = convert_date_format(result)\n",
    "    print(f\"  Base (meteo): {result.shape}\")\n",
    "    \n",
    "    merge_keys = ['adm_id', 'date']\n",
    "    \n",
    "    # Add soil moisture if available\n",
    "    if soil_moisture_df is not None:\n",
    "        soil_moisture_df = convert_date_format(soil_moisture_df)\n",
    "        sm_cols = merge_keys + [c for c in soil_moisture_df.columns \n",
    "                                if c not in merge_keys]\n",
    "        result = result.merge(soil_moisture_df[sm_cols], on=merge_keys, how='left')\n",
    "        print(f\"  + soil_moisture: {result.shape}\")\n",
    "    \n",
    "    # Add interpolated fPAR\n",
    "    if fpar_daily is not None:\n",
    "        fpar_cols = merge_keys + ['fpar']\n",
    "        result = result.merge(fpar_daily[fpar_cols], on=merge_keys, how='left')\n",
    "        print(f\"  + fpar (interpolated): {result.shape}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def add_targets_to_features(features_df, targets_df, horizon=20):\n",
    "    \"\"\"\n",
    "    Add futures price targets to feature dataframe.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAdding t+{horizon} targets to features...\")\n",
    "    \n",
    "    target_cols = ['date', f'target_price_t{horizon}', 'price_current', \n",
    "                   f'target_return_t{horizon}']\n",
    "    \n",
    "    features_df = convert_date_format(features_df)\n",
    "    result = features_df.merge(targets_df[target_cols], on='date', how='inner')\n",
    "    \n",
    "    print(f\"  Before merge: {len(features_df)} rows\")\n",
    "    print(f\"  After merge: {len(result)} rows\")\n",
    "    \n",
    "    # Drop rows without valid target\n",
    "    result = result.dropna(subset=[f'target_price_t{horizon}'])\n",
    "    print(f\"  After dropping NaN targets: {len(result)} rows\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"âœ“ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD DAILY FEATURE MATRIX\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING DAILY FEATURE MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Interpolate fPAR to daily\n",
    "print(\"\\nStep 1: Interpolating vegetation indices...\")\n",
    "fpar_daily = interpolate_to_daily(fpar_df, value_col='fpar', group_col='adm_id')\n",
    "\n",
    "# Step 2: Create futures targets\n",
    "print(\"\\nStep 2: Creating futures targets...\")\n",
    "targets = create_daily_targets(futures_clean, horizon=HORIZON)\n",
    "\n",
    "# Step 3: Merge features\n",
    "print(\"\\nStep 3: Merging features...\")\n",
    "features = merge_daily_features(meteo_df, soil_df, fpar_daily)\n",
    "\n",
    "# Step 4: Add targets\n",
    "dataset = add_targets_to_features(features, targets, horizon=HORIZON)\n",
    "\n",
    "# Step 5: Add time features\n",
    "print(\"\\nStep 5: Adding time features...\")\n",
    "dataset['year'] = dataset['date'].dt.year\n",
    "dataset['month'] = dataset['date'].dt.month\n",
    "dataset['day_of_year'] = dataset['date'].dt.dayofyear\n",
    "dataset['week_of_year'] = dataset['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ FEATURE MATRIX COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {dataset.shape}\")\n",
    "print(f\"Regions: {dataset['adm_id'].nunique()}\")\n",
    "print(f\"Unique dates: {dataset['date'].nunique()}\")\n",
    "print(f\"Date range: {dataset['date'].min()} to {dataset['date'].max()}\")\n",
    "print(f\"Years: {sorted(dataset['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exploration_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPLORATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def explore_temporal_coverage(df, date_col='date', title='Temporal Coverage'):\n",
    "    \"\"\"Analyze temporal distribution.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    dates = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    print(f\"Date range: {dates.min()} to {dates.max()}\")\n",
    "    print(f\"Total span: {(dates.max() - dates.min()).days} days\")\n",
    "    print(f\"Unique dates: {dates.nunique():,}\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    \n",
    "    # Yearly distribution\n",
    "    print(\"\\nRecords by year:\")\n",
    "    yearly = dates.dt.year.value_counts().sort_index()\n",
    "    for year, count in yearly.items():\n",
    "        print(f\"  {year}: {count:,}\")\n",
    "    \n",
    "    return dates\n",
    "\n",
    "\n",
    "def explore_spatial_coverage(df, region_col='adm_id'):\n",
    "    \"\"\"Analyze spatial distribution.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Spatial Coverage\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    regions = df[region_col].unique()\n",
    "    print(f\"Total regions: {len(regions)}\")\n",
    "    \n",
    "    # Records per region\n",
    "    region_counts = df[region_col].value_counts()\n",
    "    print(f\"\\nRecords per region:\")\n",
    "    print(f\"  Mean: {region_counts.mean():.0f}\")\n",
    "    print(f\"  Median: {region_counts.median():.0f}\")\n",
    "    print(f\"  Min: {region_counts.min()}\")\n",
    "    print(f\"  Max: {region_counts.max()}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 regions by records:\")\n",
    "    for region, count in region_counts.head().items():\n",
    "        print(f\"  {region}: {count:,}\")\n",
    "    \n",
    "    return region_counts\n",
    "\n",
    "\n",
    "def explore_feature_distributions(df, sample_size=5000):\n",
    "    \"\"\"Analyze feature distributions and correlations.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Feature Analysis\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Auto-detect numeric columns\n",
    "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = ['year', 'month', 'day_of_year', 'week_of_year']\n",
    "    feature_cols = [c for c in feature_cols if c not in exclude]\n",
    "    \n",
    "    print(f\"Analyzing {len(feature_cols)} features\")\n",
    "    \n",
    "    # Sample for speed\n",
    "    sample_df = df[feature_cols].sample(min(sample_size, len(df)), random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nFeature statistics:\")\n",
    "    stats = sample_df.describe()\n",
    "    print(stats)\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing = df[feature_cols].isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    if len(missing) > 0:\n",
    "        print(missing)\n",
    "    else:\n",
    "        print(\"  âœ“ No missing values!\")\n",
    "    \n",
    "    # Correlation with target\n",
    "    target_cols = [c for c in df.columns if 'target' in c.lower() and 'price' in c.lower()]\n",
    "    if len(target_cols) > 0:\n",
    "        print(f\"\\nCorrelation with {target_cols[0]}:\")\n",
    "        target_corr = sample_df.corrwith(df[target_cols[0]].sample(len(sample_df), random_state=RANDOM_SEED))\n",
    "        target_corr = target_corr.sort_values(ascending=False)\n",
    "        print(target_corr.head(10))\n",
    "    \n",
    "    return stats, sample_df\n",
    "\n",
    "\n",
    "def plot_futures_price_analysis(futures_df, save_dir=FIGURES_DIR):\n",
    "    \"\"\"Detailed futures price analysis.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Futures Price Analysis\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df = futures_df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Price statistics\n",
    "    print(f\"\\nPrice statistics (EUR/tonne):\")\n",
    "    print(f\"  Mean: {df['close'].mean():.2f}\")\n",
    "    print(f\"  Median: {df['close'].median():.2f}\")\n",
    "    print(f\"  Std: {df['close'].std():.2f}\")\n",
    "    print(f\"  Min: {df['close'].min():.2f}\")\n",
    "    print(f\"  Max: {df['close'].max():.2f}\")\n",
    "    \n",
    "    # Daily returns\n",
    "    df['return'] = df['close'].pct_change() * 100\n",
    "    print(f\"\\nDaily returns (%):\")\n",
    "    print(f\"  Mean: {df['return'].mean():.3f}\")\n",
    "    print(f\"  Std: {df['return'].std():.3f}\")\n",
    "    print(f\"  Min: {df['return'].min():.2f}\")\n",
    "    print(f\"  Max: {df['return'].max():.2f}\")\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Price over time\n",
    "    axes[0, 0].plot(df['date'], df['close'], linewidth=0.8)\n",
    "    axes[0, 0].set_title('Wheat Futures Price Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Price (EUR/tonne)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Price distribution\n",
    "    axes[0, 1].hist(df['close'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Price Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Price (EUR/tonne)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Daily returns\n",
    "    axes[1, 0].plot(df['date'], df['return'], linewidth=0.5, alpha=0.7)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].set_title('Daily Returns', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Return (%)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Return distribution\n",
    "    axes[1, 1].hist(df['return'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_title('Return Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Daily Return (%)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 5. Seasonal pattern\n",
    "    df['month'] = df['date'].dt.month\n",
    "    monthly_avg = df.groupby('month')['close'].mean()\n",
    "    axes[2, 0].bar(monthly_avg.index, monthly_avg.values)\n",
    "    axes[2, 0].set_title('Average Price by Month', fontsize=12, fontweight='bold')\n",
    "    axes[2, 0].set_xlabel('Month')\n",
    "    axes[2, 0].set_ylabel('Avg Price (EUR/tonne)')\n",
    "    axes[2, 0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # 6. Volatility\n",
    "    df['volatility'] = df['return'].rolling(30).std()\n",
    "    axes[2, 1].plot(df['date'], df['volatility'], linewidth=0.8)\n",
    "    axes[2, 1].set_title('30-Day Rolling Volatility', fontsize=12, fontweight='bold')\n",
    "    axes[2, 1].set_ylabel('Volatility (% std)')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'futures_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Saved plot to {save_dir / 'futures_analysis.png'}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_correlations(df, sample_size=5000, save_dir=FIGURES_DIR):\n",
    "    \"\"\"Plot correlation matrix.\"\"\"\n",
    "    feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = ['year', 'month', 'day_of_year', 'week_of_year']\n",
    "    feature_cols = [c for c in feature_cols if c not in exclude]\n",
    "    \n",
    "    # Sample for speed\n",
    "    sample_df = df[feature_cols].sample(min(sample_size, len(df)), random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr = sample_df.corr()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "                ax=ax, vmin=-1, vmax=1)\n",
    "    ax.set_title(f'Feature Correlation Matrix (n={len(sample_df):,})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved correlation matrix to {save_dir / 'correlation_matrix.png'}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_seasonal_patterns(df, feature_cols=None, save_dir=FIGURES_DIR):\n",
    "    \"\"\"Plot seasonal patterns for key features.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = ['tmin', 'tmax', 'prec', 'fpar']\n",
    "    \n",
    "    available_features = [f for f in feature_cols if f in df.columns]\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(\"âš ï¸  No features available for seasonal plotting\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(available_features), 1, \n",
    "                            figsize=(12, 3*len(available_features)))\n",
    "    if len(available_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature in enumerate(available_features):\n",
    "        monthly_pattern = df.groupby('month')[feature].agg(['mean', 'std'])\n",
    "        \n",
    "        axes[i].plot(monthly_pattern.index, monthly_pattern['mean'], \n",
    "                    marker='o', linewidth=2, markersize=6)\n",
    "        axes[i].fill_between(monthly_pattern.index,\n",
    "                            monthly_pattern['mean'] - monthly_pattern['std'],\n",
    "                            monthly_pattern['mean'] + monthly_pattern['std'],\n",
    "                            alpha=0.3)\n",
    "        axes[i].set_title(f'Seasonal Pattern: {feature}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Month')\n",
    "        axes[i].set_ylabel(feature)\n",
    "        axes[i].set_xticks(range(1, 13))\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'seasonal_patterns.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved seasonal patterns to {save_dir / 'seasonal_patterns.png'}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"âœ“ Exploration functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN COMPREHENSIVE EXPLORATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE DATA EXPLORATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"\\nDataset shape: {dataset.shape}\")\n",
    "print(f\"Memory usage: {dataset.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "# Temporal coverage\n",
    "explore_temporal_coverage(dataset, date_col='date', title='Dataset Temporal Coverage')\n",
    "explore_temporal_coverage(futures_clean, date_col='date', title='Futures Temporal Coverage')\n",
    "\n",
    "# Spatial coverage\n",
    "explore_spatial_coverage(dataset, region_col='adm_id')\n",
    "\n",
    "# Feature distributions\n",
    "stats, sample_df = explore_feature_distributions(dataset)\n",
    "\n",
    "# Futures analysis\n",
    "plot_futures_price_analysis(futures_clean)\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nðŸ“Š Generating correlation matrix...\")\n",
    "plot_feature_correlations(dataset)\n",
    "\n",
    "# Seasonal patterns\n",
    "print(\"\\nðŸ“Š Generating seasonal patterns...\")\n",
    "plot_seasonal_patterns(dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ EXPLORATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save full dataset\n",
    "output_path = OUTPUT_DIR / f\"france_wheat_daily_t{HORIZON}.csv\"\n",
    "dataset.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ“ Saved full dataset to: {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"  Rows: {len(dataset):,}\")\n",
    "print(f\"  Columns: {len(dataset.columns)}\")\n",
    "\n",
    "# Save sample for quick testing\n",
    "sample_size = min(SAMPLE_SIZE, len(dataset))\n",
    "sample = dataset.sample(sample_size, random_state=RANDOM_SEED)\n",
    "sample_path = OUTPUT_DIR / f\"france_wheat_daily_t{HORIZON}_sample.csv\"\n",
    "sample.to_csv(sample_path, index=False)\n",
    "print(f\"\\nâœ“ Saved sample to: {sample_path}\")\n",
    "print(f\"  Size: {sample_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"  Rows: {len(sample):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ ALL DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the exploration plots in:\", FIGURES_DIR)\n",
    "print(\"2. Load the processed data for modeling:\")\n",
    "print(f\"   df = pd.read_csv('{output_path}')\")\n",
    "print(\"3. Build prediction models (LSTM, Transformer, etc.)\")\n",
    "print(\"4. Evaluate on held-out test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Quick Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK DATA SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nðŸ“Š Shape: {dataset.shape}\")\n",
    "print(f\"ðŸ“… Date range: {dataset['date'].min()} to {dataset['date'].max()}\")\n",
    "print(f\"ðŸ—ºï¸  Regions: {dataset['adm_id'].nunique()}\")\n",
    "print(f\"ðŸ“† Years: {sorted(dataset['year'].unique())}\")\n",
    "\n",
    "# Features\n",
    "non_feature_cols = ['adm_id', 'date', 'year', 'month', 'day_of_year', \n",
    "                    'week_of_year', f'target_price_t{HORIZON}', \n",
    "                    'price_current', f'target_return_t{HORIZON}']\n",
    "feature_cols = [c for c in dataset.columns if c not in non_feature_cols]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target: target_price_t{HORIZON}\")\n",
    "print(f\"ðŸ“ˆ Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Sample data\n",
    "print(\"\\nðŸ“‹ Sample data:\")\n",
    "display_cols = ['date', 'adm_id', 'tmin', 'tmax', 'prec', 'fpar', \n",
    "                'price_current', f'target_price_t{HORIZON}']\n",
    "display_cols = [c for c in display_cols if c in dataset.columns]\n",
    "print(dataset[display_cols].head(10))\n",
    "\n",
    "# Data quality\n",
    "print(\"\\nâœ… Data quality:\")\n",
    "missing = dataset.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    print(\"Missing values:\")\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"âœ“ No missing values!\")\n",
    "\n",
    "# Target statistics\n",
    "print(f\"\\nðŸ“Š Target statistics:\")\n",
    "print(f\"  Current price range: â‚¬{dataset['price_current'].min():.2f} - â‚¬{dataset['price_current'].max():.2f}\")\n",
    "print(f\"  Target price range: â‚¬{dataset[f'target_price_t{HORIZON}'].min():.2f} - â‚¬{dataset[f'target_price_t{HORIZON}'].max():.2f}\")\n",
    "print(f\"  Return range: {dataset[f'target_return_t{HORIZON}'].min():.2f}% - {dataset[f'target_return_t{HORIZON}'].max():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}